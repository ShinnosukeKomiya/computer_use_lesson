このレッスンの終わりまでに、ClaudeへのAPIリクエストを自分で作れるようになります。
メッセージを効果的にフォーマットして最適なAIレスポンスを得られるようになり、システムプロンプト、最大トークン数、停止シーケンスなどのAPIパラメータをコントロールできるようになります。
それでは、コードの解説に入りましょう。
まず、Anthropic Python SDKのセットアップから始めます。
最初のステップは、単純にanthropic SDKがインストールされていることを確認することです。
pip install Anthropicを実行するだけで簡単にインストールできます。
インストールが完了したら、インポートしていきます。
具体的には、大文字の"A"のAnthropicをインポートします。
そしてそれを使って、APIリクエストを送信するためのクライアントをインスタンス化します。
では、2行目でクライアントを作成します。好きな名前を付けられますが、私は通常「client」と呼んでいます。
ここで、APIキーを明示的に渡したい場合は、anthropic_api_key=としてここにキーを渡すことができます。
しかし、省略した場合は自動的にANTHROPIC_API_KEYという環境変数を探します。
これでクライアントの準備ができました。
次のステップは、最初のリクエストを作成することです。
2つのコードセルを追加しました。
1つ目はモデル名の変数です。
このコース全体で何度もこのモデル名を使用するので、Claude-3.5-sonnet-20241022という変数に入れておきます。
これは最新のClaude 3.5 Sonnetのバージョンです。
そしてこの大きな部分が、実際にシンプルなリクエストを作成する最も重要な部分です。
クライアント変数.messages.createを使用します。
ここにはいくつかの要素がありますが、順を追って説明していきます。
まず、モデル名を渡す必要があります。これは必須です。
max_tokensも渡す必要があります。これについては後ほど説明します。
そしてmessagesも渡す必要があります。
messagesはメッセージのリストを含むリストである必要があります。
この場合は単一のメッセージで、roleがuserつまり私たちユーザーを意味します。
モデルに対してプロンプトを提供しており、contentには何らかの内容、プロンプトが設定されています。
例として、Anthropicについての俳句を書くように依頼しました。
これらのセルを実行して、特にresponse.content[0].textを出力してみましょう。
すると、Anthropicについての俳句が返ってきました:
「AIを導く 知恵と慎重な思考で よりよい未来へ」
素晴らしいですね。
では、このレスポンスオブジェクトについてもう少し詳しく見ていきましょう。
いくつかの要素があります。
まず、先ほど議論したcontentです。
contentはリストです。
[0]番目の要素のtextを見ると、実際の俳句が確認できます。
使用されたモデルも表示されています。
roleも表示されています。
元のメッセージではroleがuserでしたが、このレスポンスはroleがassistantのメッセージです。
stop_reasonも表示されており、これはモデルが生成を停止した理由を示します。
この場合は"end_turn"となっており、これは基本的に自然な停止点に達したことを意味します。
stop_sequenceはnoneです。
stop_sequenceについては後ほど詳しく説明します。
そして、usageの下では入力(プロンプト)と出力のトークン数が確認できます。
この場合、30トークンの出力がありました。
では、皆さんも試してみてください。
「Anthropicについて俳句を書いて」の代わりに、好きなプロンプトを入力してみてください。
次のステップでは、messagesリストの具体的なフォーマットについて説明します。
SDKは、max_tokensとモデル名と共に、messagesのリストを渡すように設定されています。
これまでのmessagesリストには、roleがuserに設定された単一のメッセージのみが含まれていました。
messagesフォーマットの考え方は、ClaudeへのAPIコールを会話の形式で構造化できるようにすることです。
必ずしもそのように使う必要はありませんが、何らかの会話要素を構築する場合や、以前のコンテキストを保持する必要がある場合に便利です。
現時点でmessagesについて知っておく必要があるのは、roleをuserかassistantのいずれかに設定する必要があるということです。
では、以前のコンテキストを提供してみましょう。
例えば、Claudeとスペイン語で会話をしていて、引き続きスペイン語で話してほしい場合を考えてみましょう。
messagesリストを更新して、以前の履歴を追加しました。
まずユーザーメッセージで「スペイン語だけで話してください」と伝え、次にアシスタントメッセージで「¡Hola!」と応答し、最後にユーザーメッセージがあります。
変更点は、roleがuserからassistantに、そしてまたuserに変わるだけです。
Claudeに会話の履歴を提供し、最後に「調子はどう?」と尋ねています。
これを実行すると、モデルは会話全体を考慮します。
これが完全なプロンプトとなり、スペイン語で応答が返ってきます。
これは複数のシナリオで役立ちます。
最も明白なのは、チャットボットなど会話型アシスタントの構築です。
ここでは、このmessagesフォーマットを活用したチャットボットの非常にシンプルな実装があります。
ユーザーとアシスタントのメッセージを交互に配置し、会話が進むにつれてmessagesリストを成長させていきます。
空のmessagesリストから始めて、whileループを使用します。
ユーザーが「quit」と入力しない限り永遠にループし、入力した場合はループを抜けます。
脱出方法を用意する必要がありますが、「quit」と入力しない場合は、ユーザーの入力を求め、roleがuserの新しいメッセージ辞書を作成します。
contentには、ユーザーが入力した内容(例: "hello Claude")が入ります。
これをclient.messages.createメソッドを使ってモデルに送信します。
そしてアシスタントの応答を受け取り、出力します。
さらに、そのアシスタントメッセージを新しいメッセージとしてmessagesリストに追加します。
これを繰り返します。
会話の各ターンで、このリストを何度も成長させていきます。
ユーザーメッセージを追加し、応答を得て、アシスタントメッセージを追加し、次のユーザーメッセージを受け取った時にすべてをモデルに送り返します。
では試してみましょう。
これを実行してみます。
まずは簡単な「こんにちは、私はColtです」から始めましょう。
送信すると、「こんにちはColt、私はAIアシスタントです。お会いできて嬉しいです。どのようにお手伝いできますか?」という応答が返ってきます。
完全なコンテキストが実際に保持されているかテストしてみましょう。
「私の名前は?」と聞いてみます。
送信すると、「あなたの名前はColtです。先ほど自己紹介されましたね。」と返ってきます。
もう少し興味深いことを試してみましょう。
LLMの仕組みについて教えてもらうように依頼してみました。
応答を生成してもらいます。
これはもう少し長い応答になりそうです。情報が提供されます。
そして「3番目の項目について詳しく説明して」とフォローアップします。
これは、完全な会話履歴が得られることを示すためのものです。
このメッセージ自体では、モデルにとって意味をなしませんが、送信する完全な会話履歴があれば、その3番目の箇条書きについて詳しく説明してくれます。
これがmessagesフォーマットでメッセージを送信する1つのユースケースです。
もう1つのユースケースは、モデルの口を借りる、つまり事前に入力することと呼ばれるものです。
基本的に、アシスタントメッセージを使用して、「これがあなたの応答の始まりとなる言葉です」とモデルに伝えることができます。
モデルの口を借りることができるのです。
例えば、Anthropicについての短い詩を書かせています。
これを他のものに変更してみましょう。
豚についての短い詩はどうでしょうか? いいですね。
これを実行すると、「はい、豚についての短い詩を書きましょう」のような応答が返ってくるかもしれません。
そうですね。
でも何らかの理由で、この詩を「ブー」という言葉で始めたいと思います。
どうしてもそうしたいのです。
モデルに「豚についての詩を書いて、必ず『ブー』という言葉で始めて、この前置きは不要です。すぐに詩を始めて」と指示することもできます。
しかし、もう1つの方法は、単純に「ブー」という言葉で始まるアシスタントメッセージを追加することです。
このように、roleがassistantでcontentが「ブー」という新しいメッセージを追加します。
これでモデルはこの時点から応答を始めます。
「ブー。鼻をクンクン ピンク色の丸い体 地面を楽しく転がる」という応答が返ってきます。
重要な点として、モデルはこの「ブー」という言葉を生成していないので、応答に含まれていません。
私が生成したものですが、モデルは「ブー」という言葉から始めて残りの詩を生成しました。
必要に応じて、「ブー」という言葉と残りの詩を組み合わせることもできます。
これが応答の事前入力です。
次は、APIを通じてモデルの動作を制御するために渡せるパラメータについて説明します。
最初に説明するのはmax_tokensです。
max_tokensは使用してきましたが、その機能については説明していませんでした。
簡単に言うと、max_tokensはClaudeが応答で生成すべき最大トークン数を制御します。
モデルは完全な単語や英語の単語ではなく、トークンと呼ばれる単語の断片で考えることを覚えておいてください。
また、モデルの使用量はトークン使用量に基づいて課金されます。
Claudeの場合、トークンは約3.5文字の英語に相当しますが、言語によって異なる場合があります。
このmax_tokensパラメータを使用すると、上限を設定できます。
基本的にモデルに「500トークン以上生成しないでください」と伝えることができます。
まずは1000トークンのような高い値に設定してみましょう。
モデルに大規模言語モデルについてのエッセイを書くように依頼します。
エッセイを依頼したので、おそらく多くのトークンを生成するプロンプトです。
はい、応答が返ってきました。素晴らしい。
かなり長く、非常に良いエッセイのように見えます。
では、もう一度試してみますが、今度はmax_tokensを100トークンのようなかなり短い値に設定します。
実行してみましょう。
ここで起こるのは、モデルが基本的に生成の途中で切り捨てられることです。
この100トークンの生成に達したところで切り捨てられました。
重要な点として、レスポンスオブジェクトを見ると、出力トークン数がちょうど100だったことがわかります。
その数に達して停止しました。
しかし、今回はstop_reasonが「max_tokens」となっています。
つまり、モデルは自然に停止したのではなく、stop_reasonがmax_tokensに設定されているため、max_tokensパラメータによって切り捨てられたことがわかります。
これはモデルの生成方法に影響を与えるものではありません。
モデルに「100トークンに収まる完全なエッセイを短く書いてください」とは伝えていません。
代わりに、モデルに「大規模言語モデルについてのエッセイを書いてください」と伝え、単に100トークンで切り捨てただけです。
では、なぜmax_tokensを使用するのか、あるいはなぜ低い値や高い値に変更するのでしょうか?
1つの理由は、APIコストを節約し、良いプロンプトとmax_tokensの設定の組み合わせで上限を設定することです。
例えば、チャットボットを作成する場合、エンドユーザーに5000トークンもの長いチャットボットのターンを持たせたくない場合があります。
むしろ、会話のターンが短く、チャットウィンドウに収まるようにしたい場合があるでしょう。
もう1つの理由は、速度を改善することです。
出力に含まれるトークンが多いほど、生成に時間がかかります。
次に説明するパラメータは「stop sequences」と呼ばれるものです。
これを使用すると、モデルが特定の文字列を生成した時点で停止するように、文字列のリストを提供できます。
つまり、「この単語やこの文字、このフレーズを生成したら停止してください」とモデルに伝えることができます。
これにより、単にトークン数で切り捨てるのではなく、特定の単語で出力を切り捨てることができ、より細かい制御が可能になります。
ここに、stop sequenceを使用しない例があります。
「大規模言語モデルの仕事をしたい場合に学ぶべき技術的なトピックの番号付きリストを生成してください」というプロンプトを渡します。
このプロンプトを変数に移動しました。長すぎるためです。
そして、素晴らしい番号付きリストが返ってきますが、かなり長く12個のトピックがあります。
もちろん、プロンプトを通じてモデルに上位3つか5つだけを提供するように伝えることもできますが、この例で示してみましょう。
これをコピーして複製し、今度はstop_sequences（リストで文字列を含む）を提供します。
私の場合、4を生成した後に停止してほしいとします。
つまり、「4.」と設定します。もう一度実行して、何が返ってくるか見てみましょう。
1,2,3が返ってきて、モデルは4を生成しようとして停止しました。
4は出力自体には含まれていないことに注意してください。
レスポンスオブジェクトを見ると、今回はstop_reasonが「stop_sequence」に設定されているのがわかります。
これは、モデルAPIが停止シーケンス（この場合はピリオドが続く4）に到達したため停止したことを示しています。
stop_sequencesはリストなので、必要な数だけ提供できます。
これはモデルの出力や生成を停止するタイミングを制御する1つの方法です。
より高度なプロンプト技術を学ぶときに、これの使用例をいくつか見ていきます。
次に説明するパラメータは「temperature」と呼ばれるものです。
このパラメータは、生成される応答のランダム性や創造性を制御するために使用されます。
0から1の範囲で、1のような高い値はより多様で予測不可能な応答、表現のバリエーションをもたらし、0に近い低いtemperatureはより決定論的な出力、より確率の高い表現に固執する結果となります。
このチャートは、私が実行した小さな実験の出力です。
数百のAPIリクエストが必要なため、皆さんには実行をお勧めしませんが、APIを通じてモデルに動物を1つ選ばせました。
プロンプトは「1つの動物を選んで、1語で答えてください」というようなもので、temperatureを0に設定して100回実行しました。
ご覧のように、100回の応答すべてで「キリン」という単語が返ってきました。
次に、temperatureを1に設定して同じことを行いました。
まだ「キリン」の応答が多いですが、「ゾウ」や「カモノハシ」、「コアラ」、「チーター」なども出てきます。
より多くのバリエーションが得られます。
繰り返しになりますが、temperatureが0の場合はより決定論的になる可能性が高いですが、保証はされません。temperatureが1の場合は、より多様な出力が得られます。
これを実証する関数があります。
Claudeに3回、エイリアンの惑星の名前を生成するよう依頼しています。
1語で応答するように指示し、temperatureが0の場合と1の場合でそれぞれ3回実行します。
何が起こるか見てみましょう。
この関数を実行するセルを実行します。
temperatureが0の場合、同じ惑星名が3回連続で返ってきます。Kestrax、Kestrax、Kestrax。
temperatureが1の場合は、Keylara、Kestrax、そして少し異なるスペルのKestryxが返ってきます。
このように、より多様性が得られます。
さて、リクエストの基本を見てきたので、computer useについて話を戻しましょう。
このコースで学ぶすべてのことは、何らかの形でコンピュータを構築することに関連しています。Claudeを使用するエージェントの使用に関連しています。
これは、このコースの最後に見るcomputer useクイックスタートからのコードですが、いくつかの点を強調したいと思います。
max_tokensを提供し、メッセージのリストを提供し、モデル名を提供し、後で学ぶその他の要素を含むリクエストを作成しています。
そして、会話型メッセージフォーマットも使用しています。
ご覧のように、ここにメッセージのリストがあります。
このリポジトリやファイルの上の方で定義されていますが、アシスタントの応答を追加するメッセージのリストがあります。
先ほど見たチャットボットと非常によく似ていますが、もちろんずっと複雑です。
コンピュータを使用し、スクリーンショットやツール、多くのインタラクションが含まれていますが、基本的な概念は同じです。
モデルにメッセージを送信し、アシスタントの応答を受け取ります。
それをメッセージに追加します。
十分上にスクロールすると、while Trueループの中にすべてがネストされているのが分かります。
もちろん他のロジックもたくさんありますが、結局はAPIにリクエストを送信し、max_tokensやmessagesなどを提供し、新しい応答が返ってきたらメッセージリストを更新することに帰着します。
そして、この更新され続けて成長するメッセージリストを毎回提供します。
これを何度も何度も繰り返し、このビデオで学んだすべての基本を使用します
